{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8040ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Typicality</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The actor won the battle.</td>\n",
       "      <td>Atypical (1)</td>\n",
       "      <td>2.60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The actor won the award.</td>\n",
       "      <td>Typical (7)</td>\n",
       "      <td>5.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The anchorman told the parable.</td>\n",
       "      <td>Atypical (1)</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The anchorman told the news.</td>\n",
       "      <td>Typical (7)</td>\n",
       "      <td>6.75</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The animal found the map.</td>\n",
       "      <td>Atypical (1)</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>The woman carried the bag.</td>\n",
       "      <td>Typical (7)</td>\n",
       "      <td>6.25</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>The woman opened the manhole.</td>\n",
       "      <td>Atypical (1)</td>\n",
       "      <td>2.40</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>The woman opened the bag.</td>\n",
       "      <td>Typical (7)</td>\n",
       "      <td>6.45</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>The woman painted the sign.</td>\n",
       "      <td>Atypical (1)</td>\n",
       "      <td>3.55</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>The woman painted the toenail.</td>\n",
       "      <td>Typical (7)</td>\n",
       "      <td>6.10</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>794 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Sentence    Typicality  Rating  Item\n",
       "0          The actor won the battle.  Atypical (1)    2.60     1\n",
       "1           The actor won the award.   Typical (7)    5.80     1\n",
       "2    The anchorman told the parable.  Atypical (1)    3.00     2\n",
       "3       The anchorman told the news.   Typical (7)    6.75     2\n",
       "4          The animal found the map.  Atypical (1)    2.00     3\n",
       "..                               ...           ...     ...   ...\n",
       "789       The woman carried the bag.   Typical (7)    6.25   395\n",
       "790    The woman opened the manhole.  Atypical (1)    2.40   396\n",
       "791        The woman opened the bag.   Typical (7)    6.45   396\n",
       "792      The woman painted the sign.  Atypical (1)    3.55   397\n",
       "793   The woman painted the toenail.   Typical (7)    6.10   397\n",
       "\n",
       "[794 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "plausibility = pd.read_csv('./newformat_curated_human_ratings.csv')\n",
    "plausibility['Typicality'] = plausibility['Typicality'].map({\"AT\":\"Atypical (1)\", \"T\":\"Typical (7)\"})\n",
    "plausibility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109b830c",
   "metadata": {},
   "source": [
    "# Setup Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "18817dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.output_parsers.enum import EnumOutputParser\n",
    "from langchain.output_parsers.fix import OutputFixingParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.llms import Ollama\n",
    "from tqdm import tqdm \n",
    "import time \n",
    "from enum import Enum\n",
    "# from langchain.globals import set_verbose\n",
    "# set_verbose(True)\n",
    "    \n",
    "\n",
    "class ChainManager:\n",
    "    def __init__(self):\n",
    "        self.prompt = PromptTemplate.from_template(\"Tell me a short joke about {input}\")\n",
    "        self.output_parser = StrOutputParser()\n",
    "        self.df = plausibility\n",
    "#         self.model_list = [\"llama2\"]\n",
    "        self.model_list = [\"llama2\",\"mistral\", \"orca-mini:7b\", \"qwen:7b\"]\n",
    "        self.logs = []\n",
    "        \n",
    "    def run_single_query(self, inputs, model_name, verbose, output_file_name=\"\"):\n",
    "        full_prompt = self.prompt.format(Sentence=inputs[\"Sentence\"])\n",
    "        if verbose:\n",
    "            print(\"----------------------------------------------------------------------\")\n",
    "            print(f\"model_name: {model_name}\")\n",
    "            print(f\"prompt: {full_prompt}\")\n",
    "        else:\n",
    "            self.write_string_to_buffer(\"----------------------------------------------------------------------\")\n",
    "            self.write_string_to_buffer(f\"model_name: {model_name}\")\n",
    "            self.write_string_to_buffer(f\"prompt: {full_prompt}\")\n",
    "                \n",
    "        chain = (\n",
    "            self.prompt\n",
    "            | Ollama(model=model_name)\n",
    "            | self.output_parser)\n",
    "\n",
    "        chain_of_thought = chain.invoke(inputs)\n",
    "        classifier_output = self.run_retry_classifier(chain_of_thought, 3, verbose)\n",
    "        final_answer = classifier_output[0]\n",
    "        num_retries = classifier_output[1]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Classifier finished...\\n\")\n",
    "            print(f\"chain_of_thought: {chain_of_thought}\\n\")\n",
    "            print(f\"final_answer: {final_answer}\")\n",
    "            print(f\"correct_answer: {inputs['Typicality']}\")\n",
    "        else:\n",
    "            self.write_string_to_buffer(f\"classifier finished....\\n\")\n",
    "            self.write_string_to_buffer(f\"chain_of_thought: {chain_of_thought}\\n\")\n",
    "            self.write_string_to_buffer(f\"final_answer: {final_answer}\")\n",
    "            self.write_string_to_buffer(f\"correct_answer: {inputs['Typicality']}\")\n",
    "            self.write_buffer_to_file(output_file_name)\n",
    "        return classifier_output\n",
    "    \n",
    "    def run_batch_query(self, verbose, batch_size, output_file_name=\"\"):\n",
    "        self.df = self.df.iloc[2:2+batch_size].copy()\n",
    "        input_list = self.df.to_dict('records')\n",
    "        if output_file_name != \"\":\n",
    "            self.clear_existing_file(output_file_name)\n",
    "        for model_name in self.model_list:\n",
    "            results = []\n",
    "            num_retries = []\n",
    "            for item in tqdm(input_list, desc=\"Processing queries\"):\n",
    "#                     result = self.run_single_query(item, model_name, verbose, output_file_name) \n",
    "                    result = self.run_single_self_review(item, model_name, verbose, output_file_name) \n",
    "                    results.append(result[0])\n",
    "                    num_retries.append(result[1])\n",
    "                    \n",
    "            self.df[model_name] = results \n",
    "            self.df[model_name+\"_retries\"] = num_retries \n",
    "\n",
    "    def evaluate_order(self):\n",
    "        for model_name in self.model_list:\n",
    "            binary_results = self.df[model_name].str.strip().str[0]\n",
    "            correct_predictions = (self.df['label'] == binary_results).sum()\n",
    "            total_predictions = len(self.df)\n",
    "            accuracy = correct_predictions / total_predictions\n",
    "            print(f\"{model_name}: {accuracy}\")\n",
    "        \n",
    "    \n",
    "    def verify_output(self, mcq_choice):\n",
    "        try:\n",
    "            choice_int = int(mcq_choice)\n",
    "            return 1 <= choice_int <= 7\n",
    "        except ValueError:\n",
    "            return False\n",
    "        \n",
    "    def run_classifier(self, initial_answer):\n",
    "        \n",
    "        classifier_prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are recieving an explanation from a language model about its Likert scale rating.\n",
    "        You are a numerical classifier, designed to reply numerically from 1-7. \n",
    "        You are not to explain or mention anything other than provide the numerical choice!\n",
    "        If there is not enough information, select 4. \n",
    "        Please provide only the numerical rating as your response!\n",
    "\n",
    "        Initial answer: {initial_answer}\n",
    "        Numerical answer:\n",
    "        \"\"\")\n",
    "        \n",
    "        chain = (\n",
    "            classifier_prompt\n",
    "            | Ollama(model=\"mistral\")\n",
    "            | self.output_parser\n",
    "            \n",
    "        )\n",
    "        output = chain.invoke({\"initial_answer\": initial_answer})\n",
    "        return output\n",
    "    \n",
    "    def run_retry_classifier(self, initial_answer, max_tries, verbose, output_file_name=\"\"):\n",
    "        if verbose:\n",
    "            print(\"Running classifier....\")\n",
    "        else:\n",
    "            self.write_string_to_buffer(\"Running classifier....\")\n",
    "\n",
    "        mcq_choice = 0\n",
    "        for i in range(max_tries):\n",
    "            classifier_output = self.run_classifier(initial_answer)\n",
    "            if verbose:\n",
    "                print(f\"retry_classifier_{i+1}: {classifier_output}\")\n",
    "            else:\n",
    "                self.write_string_to_buffer(f\"retry_classifier_{i+1}: {classifier_output}\")\n",
    "            mcq_choice = classifier_output.strip()[0].upper()\n",
    "            if self.verify_output(mcq_choice):\n",
    "                return (mcq_choice, i+1)\n",
    "        return (mcq_choice, max_tries)\n",
    "    \n",
    "    def run_self_review(self, question, answer, model):\n",
    "        info_retrieval_prompt = PromptTemplate.from_template(\"\"\"\n",
    "        Question: {question}\n",
    "        \n",
    "        Previous answer: {answer}\n",
    "        \n",
    "        Review your previous answer for any problems, and improve it based on your critique.\n",
    "        \"\"\")\n",
    "    \n",
    "        chain = (\n",
    "            info_retrieval_prompt\n",
    "            | model\n",
    "            | self.output_parser\n",
    "            \n",
    "        )\n",
    "        output = chain.invoke({\"question\": question, \"answer\": answer})\n",
    "        return output\n",
    "        \n",
    "       \n",
    "    def run_single_self_review(self, inputs, model_name, verbose, output_file_name=\"\"):\n",
    "        \n",
    "        llm_model = Ollama(model=model_name)\n",
    "        \n",
    "        full_prompt = self.prompt.format(Sentence=inputs[\"Sentence\"])\n",
    "        if verbose:\n",
    "            print(\"----------------------------------------------------------------------\")\n",
    "            print(f\"model_name: {model_name}\")\n",
    "            print(f\"prompt: {full_prompt}\")\n",
    "        else:\n",
    "            self.write_string_to_buffer(\"----------------------------------------------------------------------\")\n",
    "            self.write_string_to_buffer(f\"model_name: {model_name}\")\n",
    "            self.write_string_to_buffer(f\"prompt: {full_prompt}\")\n",
    "                        \n",
    "        chain = (\n",
    "            self.prompt\n",
    "            | llm_model\n",
    "            | self.output_parser)\n",
    "\n",
    "        chain_of_thought = chain.invoke(inputs)\n",
    "        \n",
    "        critique = self.run_self_review(full_prompt, chain_of_thought, llm_model)\n",
    "        \n",
    "        classifier_output = self.run_retry_classifier(critique, 3, verbose)\n",
    "        final_answer = classifier_output[0]\n",
    "        num_retries = classifier_output[1]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Classifier finished...\\n\")\n",
    "            print(f\"chain_of_thought: {chain_of_thought}\\n\")\n",
    "            print(f\"critique: {critique}\\n\")\n",
    "#             print(f\"improved_answer: {improved_answer}\")\n",
    "            print(f\"final_answer: {final_answer}\")\n",
    "            print(f\"correct_answer: {inputs['Typicality']}\")\n",
    "        else:\n",
    "            self.write_string_to_buffer(f\"classifier finished....\\n\")\n",
    "            self.write_string_to_buffer(f\"chain_of_thought: {chain_of_thought}\")\n",
    "            self.write_string_to_buffer(f\"critique: {critique}\\n\")\n",
    "#             self.write_string_to_buffer(f\"improved_answer: {improved_answer}\\n\")\n",
    "            self.write_string_to_buffer(f\"final_answer: {final_answer}\")\n",
    "            self.write_string_to_buffer(f\"correct_answer: {inputs['Typicality']}\")\n",
    "            self.write_buffer_to_file(output_file_name)\n",
    "        return  classifier_output\n",
    "    \n",
    "    def write_string_to_buffer(self, input_string):\n",
    "        self.logs.append(input_string)\n",
    "    \n",
    "    def write_buffer_to_file(self, filename):\n",
    "        with open(filename, 'a') as file:\n",
    "            for log in self.logs:\n",
    "                file.write(\"\\n\"+log)\n",
    "            self.logs = []\n",
    "        \n",
    "    def clear_existing_file(self, filename):\n",
    "         with open(filename, 'w') as file:\n",
    "            file.write(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c43b46",
   "metadata": {},
   "source": [
    "### Single Prompt + classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "baab3ece",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries: 100%|███████████████████| 350/350 [1:13:10<00:00, 12.54s/it]\n",
      "Processing queries: 100%|█████████████████████| 350/350 [49:20<00:00,  8.46s/it]\n",
      "Processing queries: 100%|█████████████████████| 350/350 [57:53<00:00,  9.92s/it]\n",
      "Processing queries: 100%|███████████████████| 350/350 [1:17:56<00:00, 13.36s/it]\n"
     ]
    }
   ],
   "source": [
    "simple_QA_chain = ChainManager()\n",
    "\n",
    "simple_QA_chain.prompt = PromptTemplate.from_template(\"\"\"\n",
    "You're tasked with evaluating the typicality of a given sentence using a Likert scale. \n",
    "1 (the sentence is very atypical or uncommon), 7 (the sentence is very typical or common). \n",
    "\n",
    "For example:\n",
    "Sentence: The actor won the battle\n",
    "Typicality rating: 3\n",
    "\n",
    "Sentence: The actor won the award\n",
    "Typicality rating: 6\n",
    "\n",
    "Please evaluate the following:\n",
    "Sentence: {Sentence}\n",
    "Typicality rating:\"\"\")\n",
    "\n",
    "simple_QA_chain.run_batch_query(False, 350, \"/Users/kohjunkai/Desktop/plausibility_simple_QA.txt\")\n",
    "simple_QA_chain.df.to_csv('/Users/kohjunkai/Desktop/plausibility_simple_QA.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf78eee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_QA_chain.evaluate_order()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad06ff4",
   "metadata": {},
   "source": [
    "### Chain of thought  + classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "221e3fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries: 100%|███████████████████| 350/350 [1:24:41<00:00, 14.52s/it]\n",
      "Processing queries: 100%|███████████████████| 350/350 [1:12:25<00:00, 12.41s/it]\n",
      "Processing queries: 100%|███████████████████| 350/350 [1:19:24<00:00, 13.61s/it]\n",
      "Processing queries: 100%|███████████████████| 350/350 [1:22:02<00:00, 14.06s/it]\n"
     ]
    }
   ],
   "source": [
    "chain = ChainManager()\n",
    "\n",
    "chain.prompt = PromptTemplate.from_template(\"\"\"\n",
    "You're tasked with evaluating the typicality of a given sentence using a Likert scale. \n",
    "1 (the sentence is very atypical or uncommon), 7 (the sentence is very typical or common). \n",
    "You are to explain your chain of thought before coming up with a typicality rating.\n",
    "\n",
    "For example:\n",
    "Sentence: The actor won the battle\n",
    "Chain of thought: The phrase implies a situation where an actor, typically known for performing in films or theater, is involved in a \"battle,\" which is less common in the context of acting. The term \"battle\" might metaphorically refer to overcoming personal challenges or competition in the industry, but it's less typical than winning awards or recognition for acting. Hence, a rating of 3 indicates that it's somewhat atypical, considering the unconventional use of \"battle\" in relation to an actor's professional achievements.\n",
    "Typicality rating: 3\n",
    "\n",
    "Sentence: The actor won the award\n",
    "Chain of thought: This statement aligns closely with common scenarios within the entertainment industry, where actors are frequently recognized for their performances through various awards. Winning an award is a typical outcome for actors who have delivered exceptional performances in their roles. Therefore, a rating of 6 is justified as it reflects a highly typical event in the context of an actor's career.\n",
    "Typicality rating: 6\n",
    "\n",
    "Please evaluate the following:\n",
    "Sentence: {Sentence}\n",
    "Chain of thought:\n",
    "Typicality rating:\"\"\")\n",
    "\n",
    "chain.run_batch_query(False, 350, \"/Users/kohjunkai/Desktop/plausibility_COT.txt\")\n",
    "chain.df.to_csv('/Users/kohjunkai/Desktop/plausibility_COT.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d612bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.df.to_csv('order_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b63eea9",
   "metadata": {},
   "source": [
    "### Self Critique + classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b096ad7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries: 100%|███████████████████| 200/200 [1:37:24<00:00, 29.22s/it]\n",
      "Processing queries: 100%|█████████████████████| 200/200 [54:08<00:00, 16.24s/it]\n",
      "Processing queries: 100%|███████████████████| 200/200 [1:10:44<00:00, 21.22s/it]\n",
      "Processing queries: 100%|███████████████████| 200/200 [1:17:36<00:00, 23.28s/it]\n"
     ]
    }
   ],
   "source": [
    "critique_chain = ChainManager()\n",
    "\n",
    "critique_chain.prompt = PromptTemplate.from_template(\"\"\"\n",
    "You're tasked with evaluating the typicality of a given sentence using a Likert scale. \n",
    "1 (the sentence is very atypical or uncommon), 7 (the sentence is very typical or common). \n",
    "You are to explain your chain of thought before coming up with a typicality rating.\n",
    "\n",
    "For example:\n",
    "Sentence: The actor won the battle\n",
    "Chain of thought: The phrase implies a situation where an actor, typically known for performing in films or theater, is involved in a \"battle,\" which is less common in the context of acting. The term \"battle\" might metaphorically refer to overcoming personal challenges or competition in the industry, but it's less typical than winning awards or recognition for acting. Hence, a rating of 3 indicates that it's somewhat atypical, considering the unconventional use of \"battle\" in relation to an actor's professional achievements.\n",
    "Typicality rating: 3\n",
    "\n",
    "Sentence: The actor won the award\n",
    "Chain of thought: This statement aligns closely with common scenarios within the entertainment industry, where actors are frequently recognized for their performances through various awards. Winning an award is a typical outcome for actors who have delivered exceptional performances in their roles. Therefore, a rating of 6 is justified as it reflects a highly typical event in the context of an actor's career.\n",
    "Typicality rating: 6\n",
    "\n",
    "Please evaluate the following:\n",
    "Sentence: {Sentence}\n",
    "Chain of thought:\n",
    "Typicality rating:\n",
    "\"\"\")\n",
    "\n",
    "critique_chain.run_batch_query(False, 200, \"/Users/kohjunkai/Desktop/plausibility_self_critique.txt\")\n",
    "critique_chain.df.to_csv('/Users/kohjunkai/Desktop/plausibility_self_critique.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
